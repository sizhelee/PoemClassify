{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc59bb75-c8e8-4784-b8a3-21eb1c6aa1ca",
   "metadata": {},
   "source": [
    "# RNN序列编码-分类期末大作业\n",
    "\n",
    "本次大作业要求手动实现双向LSTM+基于attention的聚合模型，并用于古诗作者预测的序列分类任务。**请先阅读ppt中的作业说明。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9638967f-db5b-425e-9c5b-167b2d871165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1594c12a210>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e0fee5-d222-4d07-8d6c-a5a1a97ac1b2",
   "metadata": {},
   "source": [
    "## 1. 加载数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207c3e8f-d806-4137-997b-1b3018839f4a",
   "metadata": {},
   "source": [
    "数据位于`data`文件夹中，每一行对应一个样例，格式为“诗句 作者”。下面的代码将数据文件读取到`train_data`, `valid_data`和`test_data`中，并根据训练集中的数据构造词表`word2idx`/`idx2word`和标签集合`label2idx`/`idx2label`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "408f1a0a-afb3-4c8c-968e-2a037bc6c6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {\"<unk>\": 0}\n",
    "label2idx = {}\n",
    "idx2word = [\"<unk>\"]\n",
    "idx2label = []\n",
    "\n",
    "train_data = []\n",
    "with open(\"data/train.txt\", encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        text, author = line.strip().split()\n",
    "        for c in text:\n",
    "            if c not in word2idx:\n",
    "                word2idx[c] = len(idx2word)\n",
    "                idx2word.append(c)\n",
    "        if author not in label2idx:\n",
    "            label2idx[author] = len(idx2label)\n",
    "            idx2label.append(author)\n",
    "        train_data.append((text, author))\n",
    "\n",
    "valid_data = []\n",
    "with open(\"data/valid.txt\", encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        text, author = line.strip().split()\n",
    "        valid_data.append((text, author))\n",
    "\n",
    "test_data = []\n",
    "with open(\"data/test.txt\", encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        text, author = line.strip().split()\n",
    "        test_data.append((text, author))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b1977f00-e7ad-43e8-8a5a-689fdb515c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4941 4941 5 5\n",
      "11271 1408 1410\n"
     ]
    }
   ],
   "source": [
    "print(len(word2idx), len(idx2word), len(label2idx), len(idx2label))\n",
    "print(len(train_data), len(valid_data), len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0a1eb4-e262-4bf8-9fd4-4ab52eebf476",
   "metadata": {},
   "source": [
    "**请完成下面的函数，其功能为给定一句古诗和一个作者，构造RNN的输入。** 这里需要用到上面构造的词表和标签集合，对于不在词表中的字用\\<unk\\>代替。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6969cc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label2onehot(label):\n",
    "    '''\n",
    "    input: label(tensor) N*1\n",
    "    output: onehot(tensor) N*len(word2idx)\n",
    "    '''\n",
    "    text_length = len(label)\n",
    "    onehot = torch.zeros(text_length, len(word2idx))\n",
    "    one = torch.ones_like(onehot)\n",
    "\n",
    "    onehot.scatter_(dim=1, index=label.reshape(text_length,-1).long(), src=one)\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "68e13c2f-064b-42a0-8f00-d39da3fff1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(text, author):\n",
    "    \"\"\"\n",
    "    输入\n",
    "        text: str\n",
    "        author: str\n",
    "    输出\n",
    "        x: LongTensor, shape = (1, text_length) -> (1, text_length, input_size)\n",
    "        y: LongTensor, shape = (1,)\n",
    "    \"\"\"\n",
    "    x = label2onehot(torch.tensor(list(map(lambda x: word2idx[x], text)))).unsqueeze(0)\n",
    "    y = torch.tensor(label2idx[author]).unsqueeze(0)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b2ee07-19c9-4568-aaff-019fe46a4bd3",
   "metadata": {},
   "source": [
    "## 2. LSTM算子（单个时间片作为输入）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "596a5c0b-9d6d-4166-ae52-a68f7490ca43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.f = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.o = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.g = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "    \n",
    "    def forward(self, ht, ct, xt):\n",
    "        # ht: 1 * hidden_size\n",
    "        # ct: 1 * hidden_size\n",
    "        # xt: 1 * input_size\n",
    "        input_combined = torch.cat((xt, ht), 1)\n",
    "        ft = torch.sigmoid(self.f(input_combined))\n",
    "        it = torch.sigmoid(self.i(input_combined))\n",
    "        ot = torch.sigmoid(self.o(input_combined))\n",
    "        gt = torch.tanh(self.g(input_combined))\n",
    "        ct = ft * ct + it * gt\n",
    "        ht = ot * torch.tanh(ct)\n",
    "        return ht, ct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cc96e0-1f06-43a7-aeb8-08e837c2eede",
   "metadata": {},
   "source": [
    "## 3. 实现双向LSTM（整个序列作为输入）\n",
    "\n",
    "**要求使用上面提供的LSTM算子，不要调用torch.nn.LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8a27bc3e-5c90-4af2-b842-9eb4d5fc0550",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        # TODO\n",
    "        self.fLSTM = LSTM(input_size, hidden_size)\n",
    "        self.bLSTM = LSTM(input_size, hidden_size)\n",
    "        self.register_buffer(\"_float\", torch.zeros(1, hidden_size))\n",
    "    \n",
    "    def init_h_and_c(self):\n",
    "        h = torch.zeros_like(self._float)\n",
    "        c = torch.zeros_like(self._float)\n",
    "        return h, c\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        输入\n",
    "            x: 1 * length * input_size\n",
    "        输出\n",
    "            hiddens: 1 * length * (hidden_size*2)\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "\n",
    "        length = x.shape[1]\n",
    "        hf, cf = self.init_h_and_c()\n",
    "        hb, cb = self.init_h_and_c()\n",
    "        hidden_f, hidden_b = [], []\n",
    "\n",
    "        for i in range(length):\n",
    "            hf, cf = self.fLSTM(hf, cf, x[:, i, :])\n",
    "            hb, cb = self.bLSTM(hb, cb, x[:, length-i-1, :])\n",
    "            hidden_f.append(hf)\n",
    "            hidden_b.append(hb)\n",
    "\n",
    "        hidden_b.reverse()\n",
    "        hidden_f = torch.stack(hidden_f)    # len*B*d\n",
    "        hidden_b = torch.stack(hidden_b)\n",
    "\n",
    "        hidden_f = hidden_f.reshape(-1, hidden_f.shape[2])\n",
    "        hidden_b = hidden_b.reshape(-1, hidden_b.shape[2])\n",
    "\n",
    "        hiddens = torch.hstack([hidden_f, hidden_b]) # (len*B)*(2*d)\n",
    "        hiddens = hiddens.reshape(1, length, -1)\n",
    "        \n",
    "        return hiddens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cc8fca-caff-4a6d-abf1-0ced79692cc2",
   "metadata": {},
   "source": [
    "## 4. 实现基于attention的聚合机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0122d8c3-ba8e-4b02-a9ae-8e06102ddcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        # TODO\n",
    "        self.feat2att = nn.Linear(hidden_size, hidden_size)\n",
    "        self.to_alpha = nn.Linear(hidden_size, 1, bias=False)\n",
    "\n",
    "    \n",
    "    def forward(self, hiddens):\n",
    "        \"\"\"\n",
    "        输入\n",
    "            hiddens: 1 * length * hidden_size\n",
    "        输出\n",
    "            attn_outputs: 1 * hidden_size\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        attn_f = self.feat2att(hiddens) # 1*length*hidden_size\n",
    "        dot = torch.tanh(attn_f) # 1*length*hidden_size\n",
    "        alpha = self.to_alpha(dot) # 1*length*1\n",
    "        attw = F.softmax(alpha.transpose(1, 2), dim=2) # 1*1*length\n",
    "        attn_outputs = attw @ hiddens # 1*1*hidden_size\n",
    "        attn_outputs = attn_outputs.squeeze(1) # 1*hidden_size\n",
    "\n",
    "        return attn_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b62bbf4-b565-4ec2-aebb-1d5fcbb7e048",
   "metadata": {},
   "source": [
    "## 5. 利用上述模块搭建序列分类模型\n",
    "\n",
    "参考模型结构：Embedding – BiLSTM – Attention – Linear – LogSoftmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e90edb70-7c1d-4839-9f0b-c8e8dbf9a8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, num_vocab, embedding_dim, hidden_size, num_classes):\n",
    "        \"\"\"\n",
    "        参数\n",
    "            num_vocab: 词表大小\n",
    "            embedding_dim: 词向量维数\n",
    "            hidden_size: 隐状态维数\n",
    "            num_classes: 类别数量\n",
    "        \"\"\"\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        # TODO\n",
    "        self.Encoder = BiLSTM(embedding_dim, hidden_size)\n",
    "        self.selfatt = Attention(hidden_size*2)\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Linear(hidden_size*2, num_classes), \n",
    "            nn.LogSoftmax()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        输入\n",
    "            x: 1 * length, LongTensor -> 1 * length * input_size\n",
    "        输出\n",
    "            outputs: 1 * num_classes\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        wordfeats = self.Encoder(x) # 1 * length * (hidden_size*2)\n",
    "        sentfeat = self.selfatt(wordfeats)  # 1 * (hidden_size*2)\n",
    "        outputs = self.linear_layers(sentfeat)  # 1 * num_classes\n",
    "        \n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0325df21-7e77-4883-8909-9352e90a37fe",
   "metadata": {},
   "source": [
    "## 6. 请利用上述模型在古诗作者分类任务上进行训练和测试\n",
    "\n",
    "要求选取在验证集上效果最好的模型，输出测试集上的准确率、confusion matrix以及macro-precision/recall/F1，并打印部分测试样例及预测结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "56a57606-cbbd-4532-bbac-8cb974fac21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "def train(model, train_data, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    \n",
    "    for iter, sample in enumerate(train_data):\n",
    "\n",
    "        sentence, label = sample[0], sample[1]\n",
    "        x, y = make_data(sentence, label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if iter % 500 == 0:\n",
    "            print(\"{}/{}\".format(iter, len(train_data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d99d1d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/11271\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\PKU\\21to22spring\\Python程序设计与数据科学导论-胡俊峰\\PoemClassify\\model.ipynb Cell 20'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/PKU/21to22spring/Python%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AF%BC%E8%AE%BA-%E8%83%A1%E4%BF%8A%E5%B3%B0/PoemClassify/model.ipynb#ch0000022?line=2'>3</a>\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/PKU/21to22spring/Python%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AF%BC%E8%AE%BA-%E8%83%A1%E4%BF%8A%E5%B3%B0/PoemClassify/model.ipynb#ch0000022?line=4'>5</a>\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/PKU/21to22spring/Python%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AF%BC%E8%AE%BA-%E8%83%A1%E4%BF%8A%E5%B3%B0/PoemClassify/model.ipynb#ch0000022?line=5'>6</a>\u001b[0m train(model, train_data, criterion, optimizer)\n",
      "\u001b[1;32md:\\PKU\\21to22spring\\Python程序设计与数据科学导论-胡俊峰\\PoemClassify\\model.ipynb Cell 19'\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_data, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PKU/21to22spring/Python%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AF%BC%E8%AE%BA-%E8%83%A1%E4%BF%8A%E5%B3%B0/PoemClassify/model.ipynb#ch0000017?line=11'>12</a>\u001b[0m y_pred \u001b[39m=\u001b[39m model(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PKU/21to22spring/Python%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AF%BC%E8%AE%BA-%E8%83%A1%E4%BF%8A%E5%B3%B0/PoemClassify/model.ipynb#ch0000017?line=13'>14</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(y_pred, y)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/PKU/21to22spring/Python%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AF%BC%E8%AE%BA-%E8%83%A1%E4%BF%8A%E5%B3%B0/PoemClassify/model.ipynb#ch0000017?line=14'>15</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PKU/21to22spring/Python%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AF%BC%E8%AE%BA-%E8%83%A1%E4%BF%8A%E5%B3%B0/PoemClassify/model.ipynb#ch0000017?line=15'>16</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PKU/21to22spring/Python%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AF%BC%E8%AE%BA-%E8%83%A1%E4%BF%8A%E5%B3%B0/PoemClassify/model.ipynb#ch0000017?line=17'>18</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39miter\u001b[39m \u001b[39m%\u001b[39m \u001b[39m500\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\84350\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/84350/anaconda3/envs/tensorflow/lib/site-packages/torch/_tensor.py?line=297'>298</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/84350/anaconda3/envs/tensorflow/lib/site-packages/torch/_tensor.py?line=298'>299</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    <a href='file:///c%3A/Users/84350/anaconda3/envs/tensorflow/lib/site-packages/torch/_tensor.py?line=299'>300</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    <a href='file:///c%3A/Users/84350/anaconda3/envs/tensorflow/lib/site-packages/torch/_tensor.py?line=300'>301</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/84350/anaconda3/envs/tensorflow/lib/site-packages/torch/_tensor.py?line=304'>305</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    <a href='file:///c%3A/Users/84350/anaconda3/envs/tensorflow/lib/site-packages/torch/_tensor.py?line=305'>306</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> <a href='file:///c%3A/Users/84350/anaconda3/envs/tensorflow/lib/site-packages/torch/_tensor.py?line=306'>307</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\84350\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\autograd\\__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/84350/anaconda3/envs/tensorflow/lib/site-packages/torch/autograd/__init__.py?line=150'>151</a>\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/84350/anaconda3/envs/tensorflow/lib/site-packages/torch/autograd/__init__.py?line=151'>152</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m--> <a href='file:///c%3A/Users/84350/anaconda3/envs/tensorflow/lib/site-packages/torch/autograd/__init__.py?line=153'>154</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[0;32m    <a href='file:///c%3A/Users/84350/anaconda3/envs/tensorflow/lib/site-packages/torch/autograd/__init__.py?line=154'>155</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    <a href='file:///c%3A/Users/84350/anaconda3/envs/tensorflow/lib/site-packages/torch/autograd/__init__.py?line=155'>156</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = EncoderRNN(len(word2idx), len(word2idx), 512, 5)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 1\n",
    "train(model, train_data, criterion, optimizer)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6b2b7e0d18fd3e3317af69a196e3f7a9aecdf55c7d999ac46735d442a6952a8d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
